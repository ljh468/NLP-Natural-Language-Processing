{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-2. IMDB Movie Reviews Classification  with LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4bcxQpcyV1INbWm02h1k3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xUcc4SWFCvti"},"source":["# Problem Description"]},{"cell_type":"markdown","metadata":{"id":"OCufIfi9C0Hd"},"source":["이 튜토리얼에서 시퀀스 학습을 시연하는 데 사용할 문제는 IMDB 영화 검토 감정 분류 문제이다. [IMDB movie review](http://ai.stanford.edu/~amaas/data/sentiment/) \n","\n","- 이 데이터는 스탠포드 연구진에 의해 수집되었으며 데이터의 50-50 분할이 훈련과 테스트에 사용된 2011년 논문에서 사용되었다. 88.89%의 정확도가 달성되었다.\n","- Keras는 IMDB 내장 데이터셋에 대한 액세스를 제공합니다. imdb.load_data() 함수를 사용하여 데이터 집합을 로드할 수 있습니다.\n","- 각 영화 리뷰는 단어 순서가 다양하며 각 영화 리뷰의 감정은 분류되어야 한다.\n","- 우리는 각 영화 리뷰를 단어 임베딩이라는 텍스트로 작업할 때 널리 사용되는 기술인 실제 벡터 영역으로 매핑할 것이다.\n","\n","    - 우리는 각 단어를 32길이의 실제 가치 벡터에 매핑할 것이다.\n","    - 또한 모델링에 관심이 있는 총 단어 수를 5000개의 가장 자주 사용하는 단어로 제한하고 나머지는 0개로 제한할 예정입니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS0C1EGOC4h5","executionInfo":{"status":"ok","timestamp":1632491584186,"user_tz":-540,"elapsed":688,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"b86c7848-787b-4730-cd49-f4f07575afe5"},"source":["import time\n","\n","start = time.time()\n","print('시작시간:', time.ctime(start))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["시작시간: Fri Sep 24 13:53:03 2021\n"]}]},{"cell_type":"code","metadata":{"id":"LrMGfmcODRpI","executionInfo":{"status":"ok","timestamp":1632491585939,"user_tz":-540,"elapsed":1756,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}}},"source":["import numpy\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ER33zlgeDUKI"},"source":["# Simple LSTM for Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"U4sdPIEBDXOa"},"source":["## Step 1: Data Preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1x9EDnWDY9g","executionInfo":{"status":"ok","timestamp":1632491590515,"user_tz":-540,"elapsed":4583,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"73ba8795-4689-411f-b2f7-fa9d25d8f5ff"},"source":["# load the dataset but only keep the top n words, zero the rest\n","\n","top_words = 5000 # 단어 수집 개수\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"TnfC5dNPDasj","executionInfo":{"status":"ok","timestamp":1632491592803,"user_tz":-540,"elapsed":2290,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}}},"source":["# truncate and pad input sequences\n","\n","max_review_length = 500 # 단어 범위\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yKjl32BDqGq"},"source":["## Step 2 ~ 4: Build, compile and fit the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRZTjvOMDrRC","executionInfo":{"status":"ok","timestamp":1632491724946,"user_tz":-540,"elapsed":132145,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"fb53b225-923f-4b1c-9aaa-81b15c7681d1"},"source":["# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100))  # 100 memory units (smart neurons)\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64) \n","# The model is fit for only 3 epochs because it quickly overfits the problem. "],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 500, 32)           160000    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100)               53200     \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","391/391 [==============================] - 47s 102ms/step - loss: 0.5388 - accuracy: 0.7263 - val_loss: 0.3758 - val_accuracy: 0.8416\n","Epoch 2/3\n","391/391 [==============================] - 39s 100ms/step - loss: 0.3519 - accuracy: 0.8528 - val_loss: 0.3624 - val_accuracy: 0.8478\n","Epoch 3/3\n","391/391 [==============================] - 39s 100ms/step - loss: 0.3223 - accuracy: 0.8674 - val_loss: 0.3435 - val_accuracy: 0.8562\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f589049c850>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"A8q0c_9EDtmr"},"source":["## Step 5: Evaluate the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKU3DlR9Du8w","executionInfo":{"status":"ok","timestamp":1632491745852,"user_tz":-540,"elapsed":20910,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"18950c6a-b13e-4eab-f2c9-d4ba41c98881"},"source":["# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 85.62%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8lOLgkrmDzJ6"},"source":["이 간단한 LSTM은 거의 조정되지 않고 IMDB 문제에 대해 거의 최신 결과를 얻을 수 있습니다. [state-of-the-art results](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)"]},{"cell_type":"markdown","metadata":{"id":"VW_159aND2PP"},"source":["# LSTM For Sequence Classification With Dropout"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeNTpDKfD5Io","executionInfo":{"status":"ok","timestamp":1632491864952,"user_tz":-540,"elapsed":119102,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"8ae06332-c012-4284-82a1-88c4bad7d5d6"},"source":["# LSTM with Dropout for sequence classification in the IMDB dataset\n","import numpy\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Embedding, Dropout\n","from tensorflow.keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Dropout(0.2))   # newly added\n","model.add(LSTM(100))\n","model.add(Dropout(0.2))   # newly added\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64,validation_split=0.2)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 500, 32)           0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","313/313 [==============================] - 29s 85ms/step - loss: 0.5008 - accuracy: 0.7452 - val_loss: 0.3557 - val_accuracy: 0.8496\n","Epoch 2/3\n","313/313 [==============================] - 26s 83ms/step - loss: 0.2977 - accuracy: 0.8796 - val_loss: 0.3552 - val_accuracy: 0.8514\n","Epoch 3/3\n","313/313 [==============================] - 26s 83ms/step - loss: 0.3024 - accuracy: 0.8777 - val_loss: 0.3509 - val_accuracy: 0.8484\n","782/782 [==============================] - 28s 36ms/step - loss: 0.3539 - accuracy: 0.8493\n","Accuracy: 84.93%\n"]}]},{"cell_type":"markdown","metadata":{"id":"3j1GlbISD70q"},"source":["우리는 융합이 약간 느린 추세와 이 경우 최종 정확도가 낮은 중퇴자가 교육에 원하는 영향을 미치는 것을 볼 수 있다. <br>이 모델은 몇 가지 더 많은 교육 기간을 사용할 수 있으며 더 높은 기술을 얻을 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"P3Fe9L-8D_s2"},"source":["# LSTM For Sequence Classification With [Dropout and Recurrent Dropout](https://stackoverflow.com/questions/49940280/keras-lstm-dropout-vs-recurrent-dropout)"]},{"cell_type":"markdown","metadata":{"id":"0ng6IvXuEFA5"},"source":["- LSTM을 사용하여 메모리 유닛의 입력 및 반복 연결에 드롭아웃을 정밀하고 별도로 적용할 수 있다.\n","\n","- Keras는 LSTM 계층의 매개 변수, 입력 드롭아웃을 구성하기 위한 드롭아웃 및 반복 드롭아웃을 통해 이 기능을 제공합니다.\n","\n","- 반복 드롭아웃은 반복 신경망을 위한 정규화 방법이다. 드롭아웃은 LSTM 메모리 셀(또는 GRU 상태) 업데이트에 적용된다. 즉, LSTM/GRU의 입력/업데이트 게이트를 드롭한다.\n","\n","> 드롭아웃: [0,1], 입력의 선형 변환을 위해 삭제할 단위의 비율입니다.\n","> \n","> recurrent_dropout: [0,1], recurrent 상태의 선형 변환을 위해 삭제할 단위의 비율입니다.\n",">"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wt5YJ2JoETFw","executionInfo":{"status":"ok","timestamp":1632495386070,"user_tz":-540,"elapsed":3521122,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"ad6d37b3-f357-4399-95db-5e86f1d49ad0"},"source":["import numpy\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Embedding, Dropout\n","from tensorflow.keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # newly modified\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","391/391 [==============================] - 1105s 3s/step - loss: 0.4584 - accuracy: 0.7707\n","Epoch 2/3\n","391/391 [==============================] - 1113s 3s/step - loss: 0.3063 - accuracy: 0.8769\n","Epoch 3/3\n","391/391 [==============================] - 1129s 3s/step - loss: 0.2751 - accuracy: 0.8902\n","Accuracy: 87.14%\n"]}]},{"cell_type":"markdown","metadata":{"id":"xCC6NAOWEYF7"},"source":["- LSTM 특정 드롭아웃이 계층별 드롭아웃보다 네트워크 컨버전스에 더 뚜렷한 영향을 미친다는 것을 알 수 있다.\n","\n","- 위와 같이, 에포크의 수는 일정하게 유지되었고 모델의 기술을 더 높일 수 있는지 확인하기 위해 증가할 수 있었다."]},{"cell_type":"markdown","metadata":{"id":"Y_wc3IBgEbDV"},"source":["# LSTM and Convolutional Neural Network For Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"cNgTGYnCEhw1"},"source":["- 컨볼루션 신경망은 입력 데이터의 공간 구조를 학습하는 데 탁월하다.\n","- IMDB 리뷰 데이터는 리뷰의 단어 순서로 1차원 공간 구조를 가지고 있으며 CNN은 긍정과 부정에 대해 불변 특징을 선택할 수 있다.\n","- 이렇게 학습된 공간 특성은 LSTM 계층에 의해 시퀀스로 학습될 수 있다.\n","- 임베딩 레이어 다음에 1차원 CNN 및 최대 풀링 레이어를 쉽게 추가할 수 있으며, 이 레이어는 통합된 기능을 LSTM에 제공한다.\n","    - 필터 길이가 3인 작은 32개 기능 세트를 사용할 수 있습니다.\n","    - 풀링 계층은 표준 길이 2를 사용하여 형상도 크기를 절반으로 줄일 수 있습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEfOTpjZEnPK","executionInfo":{"status":"ok","timestamp":1632495491480,"user_tz":-540,"elapsed":105415,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"653e47eb-04ec-469c-bec0-12c2dfe7aa96"},"source":["import numpy\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import MaxPooling1D\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))   # newly added\n","model.add(MaxPooling1D(pool_size=2))    # newly added\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 500, 32)           3104      \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 216,405\n","Trainable params: 216,405\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","391/391 [==============================] - 46s 43ms/step - loss: 0.5697 - accuracy: 0.7071\n","Epoch 2/3\n","391/391 [==============================] - 17s 42ms/step - loss: 0.2887 - accuracy: 0.8841\n","Epoch 3/3\n","391/391 [==============================] - 16s 42ms/step - loss: 0.2301 - accuracy: 0.9121\n","Accuracy: 87.96%\n"]}]},{"cell_type":"markdown","metadata":{"id":"gm7S-um6E6is"},"source":["교육 시간은 빨라지지만 첫 번째 예제와 유사한 결과를 얻을 수 있습니다.\n","\n","이 예제를 중퇴자를 사용하기 위해 더 확장한다면 훨씬 더 나은 결과를 얻을 수 있을 것으로 기대합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TndINBYE8TN","executionInfo":{"status":"ok","timestamp":1632495491481,"user_tz":-540,"elapsed":9,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}},"outputId":"fbd32377-14c8-4f27-a252-6a11c7f35a7b"},"source":["execution_time = time.time() - start\n","print('실행시간(분): ', execution_time/60)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["실행시간(분):  65.12580471038818\n"]}]}]}